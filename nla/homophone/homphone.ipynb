{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "executionInfo": {
     "elapsed": 2249,
     "status": "ok",
     "timestamp": 1610367408361,
     "user": {
      "displayName": "Nikhil Kothari",
      "photoUrl": "",
      "userId": "15910798946822227498"
     },
     "user_tz": -330
    },
    "id": "u8wxe46aqbyb"
   },
   "source": [
    "import os\n",
    "import re\n",
    "import bz2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import _pickle as cPickle\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from itertools import product\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The training data for homophone has two columns - \"in\" and \"out\"\n",
    "Both the columns can have the correct as well as augmented words\n",
    "\n",
    "Example - [[\"CROCIN\", \"KROSIN\"], [\"KROSIN\", \"CROCIN\"], [\"CROSIN\", \"KROSIN\"].. AND SO ON]\n",
    "Basically, all possible permutations of all possible variations of a word are taken for input and output columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cPickle.load(bz2.BZ2File('./data/homophones.pbz2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123000/123000 [00:06<00:00, 18549.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in</th>\n",
       "      <th>out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AEA</td>\n",
       "      <td>AEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEA</td>\n",
       "      <td>AAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AEA</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEA</td>\n",
       "      <td>AU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEA</td>\n",
       "      <td>AWA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    in  out\n",
       "0  AEA  AEA\n",
       "1  AEA  AAA\n",
       "2  AEA    A\n",
       "3  AEA   AU\n",
       "4  AEA  AWA"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [item for value in tqdm(data) for item in list(product(value, repeat=2))]\n",
    "data = pd.DataFrame(data, columns=['in', 'out'])\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1947,
     "status": "ok",
     "timestamp": 1610367408362,
     "user": {
      "displayName": "Nikhil Kothari",
      "photoUrl": "",
      "userId": "15910798946822227498"
     },
     "user_tz": -330
    },
    "id": "9i4zmclrlIR-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the config file..\n"
     ]
    }
   ],
   "source": [
    "# load config, if exists\n",
    "config = None\n",
    "folder_path = './checkpoint/'\n",
    "\n",
    "if os.path.isfile(folder_path + 'config.pkl'):\n",
    "    print('loading the config file..')\n",
    "    with open(folder_path + 'config.pkl', 'rb') as fp:\n",
    "        config = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15997,
     "status": "ok",
     "timestamp": 1610367422899,
     "user": {
      "displayName": "Nikhil Kothari",
      "photoUrl": "",
      "userId": "15910798946822227498"
     },
     "user_tz": -330
    },
    "id": "sLeNdvn8lIR_",
    "outputId": "6cc56ed8-a6c7-4f14-d9a1-99ed1f5f53c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading params from config file..\n",
      "parameters: max_sent_len = 37, max_output_len = 37\n"
     ]
    }
   ],
   "source": [
    "if not config:\n",
    "    max_sent_len = 0\n",
    "    max_output_len = 0\n",
    "\n",
    "    for inp, out in data.values:\n",
    "        max_sent_len = max(max_sent_len, len(inp))\n",
    "        max_output_len = max(max_output_len, len(out))\n",
    "\n",
    "    print(\n",
    "        \"original parameters - max_sent_len = {}, max_output_len = {}\".format(\n",
    "            max_sent_len, max_output_len\n",
    "        )\n",
    "    )\n",
    "\n",
    "    max_sent_len = min(50, max_sent_len)\n",
    "    max_output_len = min(50, max_output_len)\n",
    "\n",
    "    print(\n",
    "        \"updated parameters - max_sent_len = {}, max_output_len = {}\".format(\n",
    "            max_sent_len, max_output_len\n",
    "        )\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"loading params from config file..\")\n",
    "\n",
    "    max_sent_len = config[\"max_sent_len\"]\n",
    "    max_output_len = config[\"max_output_len\"]\n",
    "\n",
    "    print(\n",
    "        \"parameters: max_sent_len = {}, max_output_len = {}\".format(\n",
    "            max_sent_len, max_output_len\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 74712,
     "status": "error",
     "timestamp": 1610367484081,
     "user": {
      "displayName": "Nikhil Kothari",
      "photoUrl": "",
      "userId": "15910798946822227498"
     },
     "user_tz": -330
    },
    "id": "jPggJMn1lc1c",
    "outputId": "1ecb58ce-5ec1-41c0-8835-255510f00020"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2324276/2367742055.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data = data.apply(lambda x: x.astype(str) \\\n"
     ]
    }
   ],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "data = data.sample(frac=1)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "data = data.apply(lambda x: x.astype(str) \\\n",
    "                            .str.upper() \\\n",
    "                            .str.replace(r'[^A-Z0-9\\.%/\\s]', '') \\\n",
    "                            .str.replace(r\"\\xa0\", \" \") \\\n",
    "                            .str.strip())\n",
    "\n",
    "\n",
    "data['in'] = [v[:max_sent_len] for v in data['in']]\n",
    "data['out'] = [v[:max_output_len-2] for v in data['out']]\n",
    "\n",
    "data = data[data['in'] != '']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 73027,
     "status": "aborted",
     "timestamp": 1610367484025,
     "user": {
      "displayName": "Nikhil Kothari",
      "photoUrl": "",
      "userId": "15910798946822227498"
     },
     "user_tz": -330
    },
    "id": "_ua5GpThlc1d"
   },
   "outputs": [],
   "source": [
    "class CDataset(Dataset):\n",
    "    def __init__(self, data, config=None):\n",
    "        self.data = data\n",
    "\n",
    "        if config is None:\n",
    "            self.max_output_len = (\n",
    "                max([len(sent) for sent in self.data[\"out\"].values]) + 2\n",
    "            )\n",
    "            \n",
    "            self.max_sent_len = max([len(sent) for sent in self.data[\"in\"].values])\n",
    "\n",
    "            self.char2idx, self.idx2char = self.char2index(\n",
    "                set(pd.concat([data[\"in\"], data[\"out\"]]))\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.max_sent_len = config[\"max_sent_len\"]\n",
    "            self.max_output_len = config[\"max_output_len\"]\n",
    "\n",
    "            self.char2idx = config[\"char2idx\"]\n",
    "            self.idx2char = {v: k for k, v in self.char2idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def char2index(self, data=[]):\n",
    "        data = set([char for sent in data for char in sent])\n",
    "\n",
    "        char2idx = {c: i + 3 for i, c in enumerate(data)}\n",
    "        char2idx.update({\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2})\n",
    "\n",
    "        idx2char = {v: k for k, v in char2idx.items()}\n",
    "        return char2idx, idx2char\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # output\n",
    "        label = np.asarray(\n",
    "            [self.char2idx[\"<SOS>\"]]\n",
    "            + [self.char2idx[x] for x in self.data[\"out\"].values[idx]]\n",
    "            + [self.char2idx[\"<EOS>\"]]\n",
    "        )\n",
    "        label = torch.LongTensor(\n",
    "            np.pad(\n",
    "                label,\n",
    "                (self.char2idx[\"<PAD>\"], self.max_output_len - len(label)),\n",
    "                \"constant\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # word input and padding\n",
    "        word = np.asarray([self.char2idx[char] for char in self.data[\"in\"].values[idx]])\n",
    "        word = torch.LongTensor(\n",
    "            np.pad(\n",
    "                word,\n",
    "                (self.char2idx[\"<PAD>\"], self.max_sent_len - len(word)),\n",
    "                \"constant\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return word, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 71943,
     "status": "aborted",
     "timestamp": 1610367484041,
     "user": {
      "displayName": "Nikhil Kothari",
      "photoUrl": "",
      "userId": "15910798946822227498"
     },
     "user_tz": -330
    },
    "id": "pJSe9i1tD118"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# 700\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "BATCH_SIZE = 50000\n",
    "cdata = CDataset(data, config)\n",
    "\n",
    "# train test split\n",
    "train_split_ratio = 0.8\n",
    "\n",
    "train_split = int(train_split_ratio * len(cdata))\n",
    "# test_split = max(len(cdata) - train_split, 1000)\n",
    "test_split = len(cdata) - train_split\n",
    "# train_split = int(train_split_ratio * len(cdata)) - test_split\n",
    "\n",
    "cdata_train, cdata_test = random_split(cdata, [train_split, test_split])\n",
    "\n",
    "# train dataloader\n",
    "train_dataloader_args =  dict(shuffle=True, batch_size=BATCH_SIZE, num_workers=0)\n",
    "trainloader = DataLoader(cdata_train, pin_memory=True, **train_dataloader_args)\n",
    "\n",
    "# test dataloader\n",
    "test_dataloader_args =  dict(shuffle=True, batch_size=BATCH_SIZE, num_workers=0)\n",
    "testloader = DataLoader(cdata_test, pin_memory=True, **test_dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(folder_path + 'config.pkl') :\n",
    "    config = dict()\n",
    "  \n",
    "    config.update({'max_sent_len':cdata.max_sent_len, \n",
    "                   'max_output_len':cdata.max_output_len, \n",
    "                   'char2idx':cdata.char2idx})\n",
    "    \n",
    "    with open(folder_path + 'config.pkl', 'wb') as fp:\n",
    "        pickle.dump(config, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 69882,
     "status": "aborted",
     "timestamp": 1610367484075,
     "user": {
      "displayName": "Nikhil Kothari",
      "photoUrl": "",
      "userId": "15910798946822227498"
     },
     "user_tz": -330
    },
    "id": "EMMo0sBhkTR1"
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, dimension):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dimension, padding_idx=0)\n",
    "        self.dimension = dimension\n",
    "\n",
    "    def forward(self, input_vec):\n",
    "        return self.embedding(input_vec) * math.sqrt(self.dimension)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len, dimension, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        positional_enc = torch.zeros(max_seq_len, dimension)\n",
    "\n",
    "        den = torch.pow(\n",
    "            10000, torch.div(torch.arange(0, dimension / 2) * 2, float(dimension))\n",
    "        )\n",
    "        num = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "\n",
    "        positional_enc[:, 0::2], positional_enc[:, 1::2] = (\n",
    "            torch.sin(num / den),\n",
    "            torch.cos(num / den),\n",
    "        )\n",
    "        positional_enc = positional_enc.unsqueeze(0)\n",
    "        self.register_buffer(\"positional_enc\", positional_enc)\n",
    "\n",
    "    def forward(self, input_vec):\n",
    "        seq_len = input_vec.size(1)\n",
    "        return self.dropout(input_vec + Variable(self.positional_enc[:, :seq_len]))\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, dimension, heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.dimension = dimension\n",
    "        self.queryl = nn.Linear(dimension, dimension)\n",
    "        self.keyl = nn.Linear(dimension, dimension)\n",
    "        self.valuel = nn.Linear(dimension, dimension)\n",
    "        self.outl = nn.Linear(dimension, dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        assert self.dimension == query.size(-1)\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.queryl(query)\n",
    "        key = self.keyl(key)\n",
    "        value = self.valuel(value)\n",
    "\n",
    "        query = query.view(\n",
    "            batch_size, -1, self.heads, query.size(-1) // self.heads\n",
    "        ).transpose(1, 2)\n",
    "        key = key.view(\n",
    "            batch_size, -1, self.heads, key.size(-1) // self.heads\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        value = value.view(\n",
    "            batch_size, -1, self.heads, value.size(-1) // self.heads\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "\n",
    "        attn = self.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "\n",
    "        concat = attn.transpose(1, 2).reshape(\n",
    "            batch_size, -1, query.size(-1) * self.heads\n",
    "        )\n",
    "\n",
    "        return self.outl(concat)\n",
    "\n",
    "\n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        qk = torch.div(\n",
    "            torch.matmul(query, key.transpose(-2, -1)), math.sqrt(query.size(-1))\n",
    "        )\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            qk = qk.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        qk = nn.Softmax(dim=-1)(qk)\n",
    "        qk = self.dropout(qk) if dropout is not None else qk\n",
    "        return torch.matmul(qk, value)\n",
    "\n",
    "\n",
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, dimension, dff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.l = nn.Linear(dimension, dff)\n",
    "        self.out = nn.Linear(dff, dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_vec):\n",
    "        return self.out(self.dropout(F.relu(self.l(input_vec))))\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dimension, delta=1e-6):\n",
    "        super().__init__()\n",
    "        self.gain = nn.Parameter(torch.ones(dimension))\n",
    "        self.bias = nn.Parameter(torch.zeros(dimension))\n",
    "        self.delta = delta\n",
    "\n",
    "    def forward(self, input_vec):\n",
    "        mean = torch.mean(input_vec, dim=-1, keepdim=True)\n",
    "        std = torch.std(input_vec, dim=-1, keepdim=True) + self.delta\n",
    "        return (self.gain / std) * (input_vec - mean) + self.bias\n",
    "\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dimension, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_vec, sublayer):\n",
    "        return input_vec + self.dropout(sublayer(self.norm(input_vec)))\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, dimension, head=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadedAttention(dimension, head, dropout)\n",
    "        self.ffnn = FeedForwardNet(dimension, dropout=dropout)\n",
    "        self.resconn1 = ResidualConnection(dimension, dropout)\n",
    "        self.resconn2 = ResidualConnection(dimension, dropout)\n",
    "\n",
    "        self.norm = LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_vec, mask=None):\n",
    "        attn = self.resconn1(input_vec, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.resconn2(attn, self.ffnn), attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, number_of_layers, head, max_seq_len, dimension, dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb = Embedding(vocab_size, dimension)\n",
    "        self.penc = PositionalEncoding(max_seq_len, dimension, dropout)\n",
    "        self.enclays = nn.ModuleList(\n",
    "            [\n",
    "                copy.deepcopy(EncoderLayer(dimension, head, dropout))\n",
    "                for _ in range(number_of_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = LayerNorm(dimension)\n",
    "\n",
    "    def forward(self, input_vec, mask=None):\n",
    "        # input_vec - batch_size, max_sent_len\n",
    "        emb = self.emb(input_vec) # emb size = batch, max_sent_len, dimension\n",
    "        emb = self.penc(emb)\n",
    "\n",
    "        for layer in self.enclays:\n",
    "            emb, _ = layer(emb, mask)\n",
    "        emb = self.norm(emb)\n",
    "        return emb, _\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dimension, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ffnn = FeedForwardNet(dimension, dropout=dropout)\n",
    "        self.resconn = nn.ModuleList(\n",
    "            [copy.deepcopy(ResidualConnection(dimension, dropout)) for _ in range(3)]\n",
    "        )\n",
    "        self.attn = nn.ModuleList(\n",
    "            [\n",
    "                copy.deepcopy(MultiHeadedAttention(dimension, heads, dropout))\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_vec, encoder_output, encmask, decmask):\n",
    "        selfattn = self.resconn[0](input_vec, lambda x: self.attn[0](x, x, x, decmask))\n",
    "\n",
    "        encdecattn = self.resconn[1](\n",
    "            selfattn, lambda x: self.attn[1](x, encoder_output, encoder_output, encmask)\n",
    "        )\n",
    "\n",
    "        return self.resconn[2](encdecattn, self.ffnn)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, number_of_layers, head, max_seq_len, dimension, dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb = Embedding(vocab_size, dimension)\n",
    "        self.penc = PositionalEncoding(max_seq_len, dimension, dropout)\n",
    "        self.declays = nn.ModuleList(\n",
    "            [\n",
    "                copy.deepcopy(DecoderLayer(dimension, head, dropout))\n",
    "                for i in range(number_of_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = LayerNorm(dimension)\n",
    "\n",
    "    def forward(self, input_vec, encoder_output, encmask, decmask):\n",
    "\n",
    "        emb = self.emb(input_vec)\n",
    "        emb = self.penc(emb)\n",
    "\n",
    "        for layer in self.declays:\n",
    "            emb = layer(emb, encoder_output, encmask, decmask)\n",
    "        return self.norm(emb)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        envocab_size,\n",
    "        devocab_size,\n",
    "        enc_max_seq_len,\n",
    "        dec_max_seq_len,\n",
    "        head,\n",
    "        number_of_enc_layers,\n",
    "        number_of_dec_layers,\n",
    "        dimension,\n",
    "        dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            envocab_size, number_of_enc_layers, head, enc_max_seq_len, dimension, dropout\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            devocab_size, number_of_dec_layers, head, dec_max_seq_len, dimension, dropout\n",
    "        )\n",
    "        self.ffnn = nn.Linear(dimension, devocab_size)\n",
    "\n",
    "    def forward(self, enc_input_vec, dec_input_vec, encmask, decmask):\n",
    "\n",
    "        encout, _ = self.encoder(enc_input_vec, encmask)\n",
    "        decout = self.decoder(dec_input_vec, encout, encmask, decmask)\n",
    "\n",
    "        return F.log_softmax(self.ffnn(decout), dim=-1)\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, src, trg=None, device='cpu', pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src!=pad).unsqueeze(1)\n",
    "\n",
    "\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "\n",
    "            trg_mask = (self.trg != pad).unsqueeze(-2)\n",
    "            dimension = self.trg.size(-1)\n",
    "\n",
    "            mask = torch.tril(torch.ones(1, dimension, dimension, device=device))\n",
    "            mask[mask != 0] = 1\n",
    "            mask = Variable(mask > 0)\n",
    "\n",
    "            if self.trg.is_cuda:\n",
    "                mask.cuda()\n",
    "            self.trg_mask = trg_mask & mask\n",
    " \n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, vocab_size, pad_index, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_index = pad_index\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        prediction = prediction.contiguous().view(-1, prediction.size(-1))\n",
    "        target = target.contiguous().view(-1)\n",
    "\n",
    "        one_hot_target = torch.nn.functional.one_hot(target, num_classes=prediction.size(-1))\n",
    "        one_hot_target[:, self.pad_index] = 0\n",
    "        one_hot_target = (one_hot_target * (1 - self.alpha)) + (\n",
    "            self.alpha / (self.vocab_size - 2)\n",
    "        )\n",
    "        one_hot_target.masked_fill_((target == self.pad_index).unsqueeze(1), 0)\n",
    "\n",
    "        return F.kl_div(prediction, one_hot_target, reduction=\"sum\")\n",
    "\n",
    "    \n",
    "class CustomAdam:\n",
    "    def __init__(self, dimension, optimizer, warmup_steps=4000, step_num=0):\n",
    "        self.optimizer = optimizer\n",
    "        self.step_num = step_num\n",
    "        self.dimension = dimension\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def step(self):\n",
    "        self.step_num += 1\n",
    "        lr = self.rate()\n",
    "\n",
    "        for pg in self.optimizer.param_groups:\n",
    "            pg[\"lr\"] = lr\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self):\n",
    "        return self.dimension ** (-0.5) * min(\n",
    "            self.step_num ** (-0.5), self.step_num * self.warmup_steps ** (-1.5)\n",
    "        )\n",
    "\n",
    "        \n",
    "def init_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the new model to train further..\n"
     ]
    }
   ],
   "source": [
    "# new network definition\n",
    "INPUT_DIM = len(config[\"char2idx\"])\n",
    "OUTPUT_DIM = len(config[\"char2idx\"])\n",
    "HID_DIM = 64\n",
    "LAYERS = 2\n",
    "HEADS = 8\n",
    "DROPOUT = 0.1\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "model = Transformer(INPUT_DIM, OUTPUT_DIM, config[\"max_sent_len\"], config[\"max_output_len\"], HEADS, LAYERS, LAYERS, HID_DIM, DROPOUT).to(device)\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "# load or initialise weights of the model\n",
    "if os.path.isfile(folder_path + 'model.pt'):\n",
    "    checkpoint = torch.load(folder_path + 'model.pt', map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint, strict=False)\n",
    "    print('loading the new model to train further..')\n",
    "    \n",
    "else:\n",
    "    checkpoint = {\"epoch\": 0}\n",
    "    model.apply(init_weights)\n",
    "    print('new model weights initialised..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 142203,
     "status": "ok",
     "timestamp": 1610367311028,
     "user": {
      "displayName": "Nikhil Kothari",
      "photoUrl": "",
      "userId": "15910798946822227498"
     },
     "user_tz": -330
    },
    "id": "HqCXKO5Mp4-R",
    "outputId": "ee49a1d3-9ad8-4d6f-efea-6b9606ee8cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,164,253 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 141530,
     "status": "ok",
     "timestamp": 1610367311042,
     "user": {
      "displayName": "Nikhil Kothari",
      "photoUrl": "",
      "userId": "15910798946822227498"
     },
     "user_tz": -330
    },
    "id": "NOjCwhJ5p6yf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading optimizer state from previous run..\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(\n",
    "            model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9\n",
    "        )\n",
    "\n",
    "if checkpoint and 'optimizer_state_dict' in checkpoint:\n",
    "    print('loading optimizer state from previous run..')\n",
    "    \n",
    "    optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "optimizer = CustomAdam(\n",
    "        dimension=64,\n",
    "        warmup_steps=400,\n",
    "        optimizer=optim,\n",
    "        step_num=checkpoint['step_num'] if 'step_num' in checkpoint else 0,\n",
    "    )\n",
    "\n",
    "criterion = LabelSmoothing(vocab_size=INPUT_DIM, pad_index=0, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 140232,
     "status": "ok",
     "timestamp": 1610367311082,
     "user": {
      "displayName": "Nikhil Kothari",
      "photoUrl": "",
      "userId": "15910798946822227498"
     },
     "user_tz": -330
    },
    "id": "SGy0aaywp-t3"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(epoch, model, device, trainloader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    pbar = tqdm(trainloader)\n",
    "\n",
    "    for i, (src, trg) in enumerate(pbar, 0):\n",
    "\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        batcher = Batch(src, trg, device, 0)\n",
    "\n",
    "        output = model(batcher.src, batcher.trg, batcher.src_mask, batcher.trg_mask)\n",
    "\n",
    "        loss = criterion(output, batcher.trg_y) / batcher.ntokens\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.optimizer.zero_grad()\n",
    "\n",
    "        pred = output.argmax(-1)\n",
    "        labels = batcher.trg_y\n",
    "\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        processed += torch.count_nonzero(labels).item()\n",
    "\n",
    "        # tensorboard writing every 200 mini-batches\n",
    "        running_loss += loss.detach().item()\n",
    "        running_accuracy += round((correct / processed) * 100, 14)\n",
    "\n",
    "        # tqdm writing\n",
    "        pbar.set_description(\n",
    "            desc=\"Train Epoch - {epoch}, Mini Batch - {batch}, Train Accuracy - {accuracy}, Train Loss - {loss}\".format(\n",
    "                epoch=epoch + 1,\n",
    "                batch=i + 1,\n",
    "                accuracy=round((correct / processed) * 100, 14),\n",
    "                loss=round(loss.detach().item(), 4),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    running_loss /= len(trainloader.dataset)  \n",
    "    \n",
    "    return round((correct / processed) * 100, 14), running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 139765,
     "status": "ok",
     "timestamp": 1610367311096,
     "user": {
      "displayName": "Nikhil Kothari",
      "photoUrl": "",
      "userId": "15910798946822227498"
     },
     "user_tz": -330
    },
    "id": "kONJrqZqqAXs"
   },
   "outputs": [],
   "source": [
    "def test(model, device, testloader):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    pbar = tqdm(testloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in enumerate(pbar, 0):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            batcher = Batch(src, trg, device, 0)\n",
    "\n",
    "            output = model(batcher.src, batcher.trg, batcher.src_mask, batcher.trg_mask)\n",
    "\n",
    "            loss = criterion(output, batcher.trg_y) / batcher.ntokens\n",
    "            test_loss += loss.detach().item()\n",
    "\n",
    "            pred = output.argmax(-1)\n",
    "            labels = batcher.trg_y\n",
    "\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            processed += torch.count_nonzero(labels).item()\n",
    "\n",
    "            pbar.set_description(\n",
    "                desc=\"Test Accuracy - {accuracy}, Test Loss - {loss}\".format(\n",
    "                    accuracy=round((correct / processed) * 100, 14),\n",
    "                    loss=round(loss.detach().item(), 4),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        test_loss /= len(testloader.dataset)\n",
    "\n",
    "        return round((correct / processed) * 100, 14), test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 139233,
     "status": "ok",
     "timestamp": 1610367311099,
     "user": {
      "displayName": "Nikhil Kothari",
      "photoUrl": "",
      "userId": "15910798946822227498"
     },
     "user_tz": -330
    },
    "id": "3msyE3h4IFlb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "char2idx = config['char2idx']\n",
    "idx2char = {v:k for k,v in char2idx.items()}\n",
    "max_sent_len = config['max_sent_len']\n",
    "\n",
    "\n",
    "def seq2seq(text_list):\n",
    "    model.eval()\n",
    "\n",
    "    text_ = [[char2idx[char] for char in query][:max_sent_len] for query in text_list]\n",
    "    \n",
    "    text = []\n",
    "    for t in text_:\n",
    "        text.append(t + [0] * (max_sent_len - len(t)))\n",
    "    \n",
    "    text = torch.from_numpy(np.array(text)).to(torch.int64).to(device)\n",
    "     \n",
    "    src_mask = (text != 0).unsqueeze(1)\n",
    "\n",
    "    decoder_inp = [char2idx[\"<SOS>\"]]\n",
    "\n",
    "    for i in range(cdata.max_output_len):\n",
    "        trg_tensor = torch.LongTensor(decoder_inp).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = (trg_tensor != 0).unsqueeze(-2)\n",
    "        dimension = trg_tensor.size(-1)  # get seq_len for matrix\n",
    "\n",
    "        mask = torch.tril(torch.ones(1, dimension, dimension, device=device))\n",
    "        mask[mask != 0] = 1\n",
    "        mask = Variable(mask > 0)\n",
    "\n",
    "        if trg_tensor.is_cuda:\n",
    "            mask.cuda()\n",
    "        trg_mask = trg_mask & mask\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.forward(text, trg_tensor, src_mask, trg_mask)\n",
    "\n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        decoder_inp.append(pred_token)\n",
    "\n",
    "        if pred_token == char2idx[\"<EOS>\"]:\n",
    "            break\n",
    "\n",
    "    return ''.join([idx2char[i] for i in decoder_inp][1:])\n",
    "  \n",
    "# # 'DETTOL ORIGINAL HAND WASH REFILL PACKET 175 ML RECKITT BENCKISER INDIA LIMITED<EOS>'\n",
    "seq2seq(['Reckitt'.upper()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 186207,
     "status": "error",
     "timestamp": 1610367359732,
     "user": {
      "displayName": "Nikhil Kothari",
      "photoUrl": "",
      "userId": "15910798946822227498"
     },
     "user_tz": -330
    },
    "id": "Rg1xc1u7lISK",
    "outputId": "99d642f4-318f-4779-81e3-4483ec957552"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "stats_file = folder_path + 'stats.csv'\n",
    "stats = pd.read_csv(stats_file).values.tolist() if os.path.exists(stats_file) else []\n",
    "\n",
    "for epoch in range(checkpoint['epoch']+1, EPOCHS):\n",
    "    print(seq2seq(['Makhana'.upper()]))\n",
    "    print(seq2seq(['Pampers'.upper()]))\n",
    "    print(seq2seq(['crocin'.upper()]))\n",
    "    print(seq2seq(['hajmola'.upper()]))\n",
    "    print(seq2seq(['rin'.upper()]))\n",
    "\n",
    "    # model training\n",
    "    acc, loss = train(epoch, model, device, trainloader, optimizer)\n",
    "        \n",
    "    # model testing\n",
    "    tacc, tloss = test(model, device, testloader)\n",
    "    \n",
    "    stats.append([epoch, acc, loss, tacc, tloss])\n",
    "    \n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(), \n",
    "            \"optimizer_state_dict\": optimizer.optimizer.state_dict(),\n",
    "            \"step_num\": optimizer.step_num,\n",
    "        },\n",
    "        folder_path + \"model.pt\",\n",
    "    )\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        path_ = folder_path + \"backup/\" + str(epoch)\n",
    "        Path(path_).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.optimizer.state_dict(),\n",
    "                \"step_num\": optimizer.step_num,\n",
    "            },\n",
    "            path_ + \"/model.pt\",\n",
    "        )       \n",
    " \n",
    "    pd.DataFrame(stats, columns=['epoch', 'train_accuracy', 'train_loss', 'test_accuracy', 'test_loss']).to_csv(stats_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "case.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
